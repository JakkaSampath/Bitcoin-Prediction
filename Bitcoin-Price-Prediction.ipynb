{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JakkaSampath/Bitcoin-Prediction/blob/main/Bitcoin-Price-Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "40hzx6bAwm3F"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn import metrics\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuQxc5IXrqLZ"
      },
      "source": [
        "Importing Dataset\n",
        "\n",
        "The dataset we will use here to perform the analysis and build a predictive model is Bitcoin Price data. We will use OHLC('Open', 'High', 'Low', 'Close') data from 17th July 2014 to 29th December 2022 which is for 8 years for the Bitcoin price."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CT4OOL3wm5w"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('bitcoin.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "PSLSh2K0sU4G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QY5qzhzIwm8d"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8ihOVlowm-0"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYzPenhMwnBT"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgIOGa8SrqLa"
      },
      "source": [
        "Exploratory Data Analysis\n",
        "\n",
        "EDA is an approach to analyzing the data using visual techniques. It is used to discover trends, and patterns, or to check assumptions with the help of statistical summaries and graphical representations.\n",
        "\n",
        "While performing the EDA of the Bitcoin Price data we will analyze how prices of the cryptocurrency have moved over the period of time and how the end of the quarters affects the prices of the currency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2W1KUnxOwnE4"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(df['Close'])\n",
        "plt.title('Bitcoin Close price.', fontsize=15)\n",
        "plt.ylabel('Price in dollars.')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_UtnoSnxrhC"
      },
      "outputs": [],
      "source": [
        "df[df['Close'] == df['Adj Close']].shape, df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lY6ZimOcxrjh"
      },
      "outputs": [],
      "source": [
        "df = df.drop(['Adj Close'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RH9mFiMRxrmA"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGPVaHeOxrog"
      },
      "outputs": [],
      "source": [
        "features = ['Open', 'High', 'Low', 'Close']\n",
        "\n",
        "plt.subplots(figsize=(20,10))\n",
        "for i, col in enumerate(features):\n",
        "  plt.subplot(2,2,i+1)\n",
        "  sn.distplot(df[col])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26fpXPNrxrq-"
      },
      "outputs": [],
      "source": [
        "plt.subplots(figsize=(20,10))\n",
        "for i, col in enumerate(features):\n",
        "  plt.subplot(2,2,i+1)\n",
        "  sn.boxplot(df[col], orient='h')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtiqKFzKrqLb"
      },
      "source": [
        "Feature Engineering\n",
        "\n",
        "Feature Engineering helps to derive some valuable features from the existing ones. These extra features sometimes help in increasing the performance of the model significantly and certainly help to gain deeper insights into the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RT3DjQu9xrtk"
      },
      "outputs": [],
      "source": [
        "splitted = df['Date'].str.split('-', expand=True)\n",
        "\n",
        "df['year'] = splitted[0].astype('int')\n",
        "df['month'] = splitted[1].astype('int')\n",
        "df['day'] = splitted[2].astype('int')\n",
        "\n",
        "# Convert the 'Date' column to datetime objects\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ipt3F_tmxrw7"
      },
      "outputs": [],
      "source": [
        "data_grouped = df.groupby('year').mean()\n",
        "plt.subplots(figsize=(20,10))\n",
        "for i, col in enumerate(['Open', 'High', 'Low', 'Close']):\n",
        "  plt.subplot(2,2,i+1)\n",
        "  data_grouped[col].plot.bar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlpphuPgyivh"
      },
      "outputs": [],
      "source": [
        "df['is_quarter_end'] = np.where(df['month']%3==0,1,0)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFhgqvagymTb"
      },
      "outputs": [],
      "source": [
        "df['open-close']  = df['Open'] - df['Close']\n",
        "df['low-high']  = df['Low'] - df['High']\n",
        "df['target'] = np.where(df['Close'].shift(-1) > df['Close'], 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTdiY8QEynZA"
      },
      "outputs": [],
      "source": [
        "plt.pie(df['target'].value_counts().values,\n",
        "        labels=[0, 1], autopct='%1.1f%%')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybmpDy8yynbX"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "sn.heatmap(df.corr() > 0.9, annot=True, cbar=False)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3PRmKy1yneS"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming df is already defined\n",
        "features = df[['open-close', 'low-high', 'is_quarter_end']]\n",
        "target = df['target']\n",
        "\n",
        "# Scaling the features\n",
        "scaler = StandardScaler()\n",
        "features = scaler.fit_transform(features)\n",
        "\n",
        "# Split the data into training and validation (test) sets\n",
        "X_train, X_valid, Y_train, Y_valid = train_test_split(features, target, test_size=0.3, random_state=42)\n",
        "\n",
        "# 'test_size=0.3' means 30% of the data will be used for testing, and 70% for training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YK0esx4rqLc"
      },
      "source": [
        "Model Development and Evaluation\n",
        "\n",
        "Now is the time to train some state-of-the-art machine learning models(Logistic Regression, Support Vector Machine, XGBClassifier), and then based on their performance on the training and validation data we will choose which ML model is serving the purpose at hand better.\n",
        "\n",
        "For the evaluation metric, we will use the ROC-AUC curve but why this is because instead of predicting the hard probability that is 0 or 1 we would like it to predict soft probabilities that are continuous values between 0 to 1. And with soft probabilities, the ROC-AUC curve is generally used to measure the accuracy of the predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYUXSNl9ynhw"
      },
      "outputs": [],
      "source": [
        "models = [LogisticRegression(), SVC(kernel='poly', probability=True), XGBClassifier()]\n",
        "\n",
        "for i in range(3):\n",
        "  models[i].fit(X_train, Y_train)\n",
        "\n",
        "  print(f'{models[i]} : ')\n",
        "  print('Training Accuracy : ', metrics.roc_auc_score(Y_train, models[i].predict_proba(X_train)[:,1]))\n",
        "  print('Validation Accuracy : ', metrics.roc_auc_score(Y_valid, models[i].predict_proba(X_valid)[:,1]))\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTbkHg3WrqLd"
      },
      "source": [
        "\n",
        "Among the three models, we have trained XGBClassifier has the highest performance but it is pruned to overfitting as the difference between the training and the validation accuracy is too high. But in the case of the Logistic Regression, this is not the case.\n",
        "Now let's plot a confusion matrix for the validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19ZhRhdwz_eA"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "ConfusionMatrixDisplay.from_estimator(models[0], X_valid, Y_valid, cmap='Blues')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}